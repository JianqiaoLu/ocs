\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage[]{algorithm2e}
\usepackage{booktabs}
\title{Balance plus Sampling without Replacement  Algorithm}
\author{tjlujianqiao }
\date{June 2022}

\begin{document}
\section{Intro.}
\subsection{Unweighted sampling without replacement}
\begin{tabular}{l}
\toprule  %添加表格头部粗线
\textbf{Algorithm} Unweighted Sampling without Replacement\\
\midrule  %添加表格中横线
\textbf{State variables:}\\
\quad  weight mass $x_e^t$ of element e in the round $t$.
\\
\quad  Cumulative mass $y_e^t$  of element e up to $t$.
\\
\textbf{For each round t:}\\
\quad 1. If all elements in $\mathcal{E}^{t}$ have been selected, select arbitrarily, e.g., uniformly at random.
\\
\quad 2. Otherwise, select an unselected $e \in \mathcal{E}^{t}$ with probability proportional $x_{e}^{t}$
\\
\bottomrule %添加表格底部粗线
In the unweighted version, 
\section{Result}
\textbf { Corollary } 1. \textit{Unbounded Balance with the Sampling without Replacement Algorithm is at least 0.513-competitive for the unweighted online bipartite matching}
\\
\textit{Proof.}
Denote $\mathcal{U}^{t}$ as the set of unselected elements after round t and $\mathcal{E}^{\prime} \subseteq \mathcal{U}^{t}$ as the subset of unselected elements, the following is to be proved by induction on $t\in  [0,d]$:
\begin{equation*}
\begin{aligned}
\operatorname{Pr}\left[\mathcal{E}^{\prime} \subseteq \mathcal{U}^{t}\right] \leq \prod_{e \in \mathcal{E}^{\prime}} p\left(y_{e}^{t}\right) 
\end{aligned}
\end{equation*}
It is trival at $t = 0$ since  $y_e^0=0,\ p(0)=1$.
\\
Suppose that the inequality holds for $t -1$ rounds.
Let $\bar{X}_{e}^{t}$ indicate whether element e is unselected after round t, and define $\bar{X}_{\mathcal{E}^{\prime}}^{t}=\prod_{e \in \mathcal{E}^{\prime}} \bar{X}_{e}^{t}$, $\bar{X}^{t} =
\left(\bar{X}_{e}^{t}\right)_{e \in \mathcal{E}}$
\begin{equation*}
\begin{aligned} \operatorname{Pr}\left[\mathcal{E}^{\prime} \subseteq \mathcal{U}^{t}\right] &=\mathbf{E} \bar{X}_{\mathcal{E}^{\prime}}^{t} 
\\
&=\mathbf{E}_{\bar{X}^{t-1}}\left[\bar{X}_{\mathcal{E}^{\prime}}^{t-1}\left(1-\frac{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} \bar{X}_{e}^{t-1}}{\sum_{e \in \mathcal{E}}  x_{e}^{t} \bar{X}_{e}^{t-1}}\right)\right] 
\\
&=
\mathbf{E}_{\bar{X}^{t-1}}\left[\bar{X}_{\mathcal{E}^{\prime}}^{t-1}\left(\frac{\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \bar{X}_{\mathcal{E}^{\prime}}^{t-1}}
{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} \bar{X}_{\mathcal{E}^{\prime}}^{t-1} 
+
\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \bar{X}_{\mathcal{E}^{\prime}\cup \{e\}}^{t-1}
}\right)\right] 
\end{aligned}
\end{equation*}
\\
The third equality is because of $\bar{X}^2_e = \bar{X}_e$ and $\bar{X}_{\mathcal{E}^{\prime}}^{t}=\prod_{e \in \mathcal{E}^{\prime}} \bar{X}_{e}^{t}$.
Due to the concavity of $f(x, y)=\frac{x y}{x+y}$, it follows from the Jensen's inequality:
\begin{equation*}
\begin{aligned}
\operatorname{Pr}\left[\mathcal{E}^{\prime} \subseteq \mathcal{U}^{t}\right] 
\leq
 \frac{\mathbf{E} \bar{X}_{\mathcal{E}^{\prime}}^{t-1} \sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \mathbf{E} \bar{X}_{\mathcal{E}^{\prime} \cup\{e\}}^{t-1}}{\sum_{e \in \mathcal{E}^{\prime}} x_{e}^{t} \mathbf{E} \bar{X}_{\mathcal{E}^{\prime}}^{t-1}+\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \mathbf{E} \bar{X}_{\mathcal{E}^{\prime} \cup\{e\}}^{t-1}}
\end{aligned}
\end{equation*}
\\
By the inductive hypothesis, it is as:
\begin{equation*}
\begin{aligned}
\operatorname{Pr}\left[\mathcal{E}^{\prime} \subseteq \mathcal{U}^{t}\right] & \leq \frac{\prod_{e \in \mathcal{E}^{\prime}} p\left(y_{e}^{t-1}\right) \sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \prod_{e^{\prime} \in \mathcal{E}^{\prime} \cup\{e\}} p\left(y_{e^{\prime}}^{t-1}\right)}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} \prod_{e \in \mathcal{E}^{\prime}} p\left(y_{e}^{t-1}\right)+\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} \prod_{e^{\prime} \in \mathcal{E}^{\prime} \cup\{e\}} p\left(y_{e^{\prime}}^{t-1}\right)} 
\\
&=\prod_{e \in \mathcal{E}^{\prime}} p\left(y_{e}^{t-1}\right) \frac{\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} p\left(y_{e}^{t-1}\right)}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t}+\sum_{e \notin \mathcal{E}^{\prime}}  x_{e}^{t} p\left(y_{e}^{t-1}\right)} 
\\
\end{aligned}
\end{equation*}
\\
Thus it remains to prove 
\begin{equation*}
\begin{aligned}
\frac{ \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(y_e^{t-1})}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} +\sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(y_e^{t-1})} 
\leq
\prod_{ e \in \mathcal{E}^{\prime}} \frac{p(y_e^{t-1} + x_e^t)}{p(y_e^{t-1})}
\end{aligned}
\end{equation*}
Due to the construction of Balance algorithm, it is to prove 
\begin{equation*}
\begin{aligned}
\frac{ \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(y_e^{t-1})}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} +\sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(y_e^{t-1})} &\leq  \prod_{ e \in \mathcal{E}^{\prime}} \frac{p(y_e^{t-1} + x_e^t)}{p(y_e^{t-1})}
\\
s.t., y_e^{t-1} + x_e^t  &= Y,  \forall t, x_e^{t} > 0
\\
\sum_{e } x_e^t &= 1
\end{aligned}
\end{equation*}
Assuming $\sum_{e \notin \mathcal{E}^{\prime}} $ is fixed, LHS increases when  $\sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(Y-x_e^t)$ increases, maximum LHS =
\begin{equation*}
    \begin{aligned}
        \frac{ \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(\max\{0,Y-\sum_{e \notin \mathcal{E}^{\prime}}x_e^t\})}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} + 
 \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(\max\{0,Y-\sum_{e \notin \mathcal{E}^{\prime}}x_e^t\})
}
    \end{aligned}
\end{equation*}
The RHS of the above inequality is: 
\begin{equation*}
    \begin{aligned}
        \exp\left(\sum_{e\in\mathcal{E'}}x_e^t\cdot\frac{\log p(Y)-\log p(Y-x_e^t)}{x_e^t}\right)
    \end{aligned}
\end{equation*}
Assuming $\sum_{e \in \mathcal{E}^{\prime}} $ is fixed and $\log p(y)$ is concave, minimum RHS = 
\begin{equation*}
    \begin{aligned}
        \exp\left(\sum_{e\in\mathcal{E'}}x_e^t\frac{p'(Y)}{p(Y)}\right)
    \end{aligned}
\end{equation*}
Thus, it remains to prove 
\begin{equation*}
   \begin{aligned}
    \frac{ \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(\max\{0,Y-\sum_{e \notin \mathcal{E}^{\prime}}x_e^t\})}{\sum_{e \in \mathcal{E}^{\prime}}  x_{e}^{t} + 
 \sum_{e \notin \mathcal{E}^{\prime}} x_{e}^{t} p(\max\{0,Y-\sum_{e \notin \mathcal{E}^{\prime}}x_e^t\})
} \leq \exp\left(\sum_{e\in\mathcal{E'}}x_e^t\frac{p'(Y)}{p(Y)}\right)
   \end{aligned}    
\end{equation*}
Let $\sum_{e \notin \mathcal{E}^{\prime}}x_e^t =  1-X$, $\sum_{e \mathcal{E}^{\prime}}x_e^t =X$, it is to prove:
\begin{equation*}
    \begin{aligned}
        \frac{Xp(Y-X)}{1- X+Xp(Y-X)} \leq \exp((1-X)\frac{p'(Y)}{p(Y)}) 
    \end{aligned}
\end{equation*}
It is equivalent to prove
\begin{equation*}
    \begin{aligned}
       \frac{\log\left(\frac{Xp(Y-X)}{1- X+Xp(Y-X)}\right)}
       {1 -X} 
       \leq 
       \frac{p'(Y)}{p(Y)}
    \end{aligned}
\end{equation*}
The LHS can get its minimum when $X = 1$ if $X \in [0,1]$ and $Y \geq 0$, it is as:
\begin{equation*}
    \begin{aligned}
        -\frac{1}{p(\max\left\{Y-1,0\right\})}
        \leq 
        \frac{p'(Y)}{p(Y)}
    \end{aligned}
\end{equation*}
It holds true
When $Y\leq 1$, $p(Y)=e^{-Y}.$ and $1\leq Y$, $p(Y)=e^{-e^{Y-1}}$. 
% Based on \textbf{Theorem 45}.\cite{gao2021focs},  the approximation ration is $\int_{0}^{\infty} e^{-z}(1-p(z)) d z \approx 0.513$
% \bibliographystyle{IEEEtran}
% \bibliography{citation}
\end{document}

